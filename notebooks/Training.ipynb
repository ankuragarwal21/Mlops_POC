{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dacef87-a082-4448-baea-0a9d4b93e130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Installing all required libraries and dependencies\n",
    "# -------------------------------\n",
    "%pip install --force-reinstall --no-cache-dir \\\n",
    "  numpy==1.25.2 \\\n",
    "  pandas==2.1.3 \\\n",
    "  scikit-learn==1.3.2 \\\n",
    "  jax==0.4.25 \\\n",
    "  jaxlib==0.4.25 \\\n",
    "  numpyro==0.13.2 \\\n",
    "  lightweight-mmm==0.1.9 \\\n",
    "  mlflow \\\n",
    "  openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec8628e4-68d5-4ee9-af64-df1dc29ff450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Restarting python after installing new libraries\n",
    "# -------------------------------\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee6f5c5-eb89-456e-b1c5-23aeb6c0c161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Importing all relevant libraries\n",
    "# -------------------------------\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "import numpyro\n",
    "import warnings\n",
    "import itertools\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow.pyfunc\n",
    "import mlflow.sklearn \n",
    "import jax.numpy as jnp\n",
    "import databricks.connect as db_connect\n",
    "import mlflow.tracking._model_registry.utils\n",
    "from sklearn import datasets\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet, lasso_path, enet_path\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "\n",
    "# libaries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae3f20b-d190-4b67-b147-37675f4099bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Import the relevant modules of the lightweight mmm library\n",
    "# -------------------------------\n",
    "from lightweight_mmm import lightweight_mmm\n",
    "from lightweight_mmm import optimize_media\n",
    "from lightweight_mmm import plot\n",
    "from lightweight_mmm import preprocessing\n",
    "from lightweight_mmm import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab4b7e6-42e6-4b3a-920b-4e42f459b898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Load data from snowflake and converting it into pandas dataframe\n",
    "# -------------------------------\n",
    "path=\"/Workspace/Users/ankur242199@exlservice.com/Data_Files/\"\n",
    "exp_path=\"/Workspace/Users/ankur242199@exlservice.com/experiments/\"\n",
    "data=pd.read_csv(path+ \"POC_Clean_Dataset.csv\")\n",
    "Train = pd.read_csv(path + \"Train_Dataset.csv\")\n",
    "Test = pd.read_csv(path + \"Test_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66263aa3-121b-48af-92af-1cedd5d7f3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Train.shape, Test.shape\n",
    "\n",
    "#drop the date column as its string column\n",
    "data.drop(columns=['WEEK_START'], inplace=True)\n",
    "\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363f0315-05ed-44a5-b108-74a84d7108d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_elastic (x,y,l1_ratio):\n",
    "    eps= 5e-3\n",
    "    alpha_enet, coef_enet, _ = enet_path(x, y, eps=eps,l1_ratio=l1_ratio)\n",
    "    global image\n",
    "\n",
    "    fig = plt.figure()\n",
    "    az = plt.gca()\n",
    "    colors = cycle(['b', 'r', 'g', 'c', 'k'])\n",
    "    neg_alpa_ents = -np.log10(alpha_enet)\n",
    "\n",
    "    for coef_enet,c in zip(alpha_enet,colors):\n",
    "        l1=plt.plot(neg_alpa_ents,coef_enet,c=c,linestyle='--')\n",
    "    plt.xlabel('Log Alpha')\n",
    "    plt.ylabel('Cofficients')\n",
    "    title = \"ElasticNet path by alpha for l1_ratio = \" + str(l1_ratio) \n",
    "    plt.title(title)\n",
    "    plt.axis('tight')\n",
    "    image= fig\n",
    "    fig.savefig(exp_path+ 'elasticnet_path.png')\n",
    "    plt.close(fig)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a1049df-2f2a-4046-97c3-97957e868bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def metric_for_model(actual,pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual,pred))\n",
    "    mae = mean_absolute_error(actual,pred)\n",
    "    r2 = r2_score(actual,pred)\n",
    "    return rmse,mae,r2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c686e1-932a-411b-acfa-6cc428ba8cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_sales_order_model(data,l1_alpha,l1_ratio):\n",
    "\n",
    "    import shutil\n",
    "    import os\n",
    "    \n",
    "    # Set the registry URI to use Unity Catalog\n",
    "    #mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "    train,test = train_test_split(data)\n",
    "    train_x = train.drop(['ORDERS'],axis=1)\n",
    "    train_y = train[['ORDERS']]\n",
    "    test_x = test.drop(['ORDERS'],axis=1)  \n",
    "    test_y = test[['ORDERS']]\n",
    "    print(\"shape_x:\", train_x.shape, test_x.shape)\n",
    "    print(\"shape_y:\", train_y.shape, test_y.shape)\n",
    "\n",
    "    \n",
    "    #check if user has not given any alpha value \n",
    "    if float(l1_alpha) is None:\n",
    "        alpha= 0.05\n",
    "    else:\n",
    "        alpha = float(l1_alpha)\n",
    "        \n",
    "    #check if user has not given any l1_ratio value \n",
    "    if float(l1_ratio) is None:\n",
    "        ratio= 0.5\n",
    "    else:\n",
    "        ratio = float(l1_ratio)\n",
    "\n",
    "    CATALOG_NAME = \"dmpipeline-dev\"\n",
    "    SCHEMA_NAME = \"poc\"\n",
    "    MODEL_NAME = \"sales_order_model\"\n",
    "    registered_model_name=f\"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}\"\n",
    "    print(\"Registered model name:\", registered_model_name)\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        lr= ElasticNet(alpha=alpha,l1_ratio=ratio,random_state=42) #l1_ratio instead of l1_score\n",
    "        lr.fit(train_x,train_y)\n",
    "        lr_predict = lr.predict(test_x)\n",
    "        rmse,mae,r2 = metric_for_model(test_y,lr_predict)\n",
    "        print(\"ElasticNet Model(alpha=%f, l1_ratio=%f\" % (alpha,l1_ratio)) #l1_ratio instead of l1_score\n",
    "        print(\"RMSE:\",rmse)\n",
    "        print(\"MAE:\",mae)\n",
    "        print(\"R2:\",r2)\n",
    "\n",
    "        input_example = test_x.iloc[[0]]\n",
    "\n",
    "        mlflow.log_param(\"alpha\",alpha)\n",
    "        mlflow.log_param(\"l1_ratio\",l1_ratio)\n",
    "        mlflow.log_param(\"RMSE\",rmse)\n",
    "        mlflow.log_param(\"MAE\",mae)\n",
    "        mlflow.log_param(\"R2\",r2)\n",
    "        mlflow.sklearn.log_model(lr, \"model\", input_example=input_example,\n",
    "        registered_model_name=f\"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}\")\n",
    "\n",
    "    \n",
    "    # Set the experiment\n",
    "        experiment_name = \"/Shared/name_of_experiment/\"\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Define the model path\n",
    "        model_path = \"/Workspace/Users/ankur242199@exlservice.com/experiments/model-%f-%f\" % (alpha, l1_ratio)\n",
    "\n",
    "    # Remove existing contents if the path exists\n",
    "        if os.path.exists(model_path):\n",
    "            shutil.rmtree(model_path)\n",
    "\n",
    "# Save the model\n",
    "        mlflow.sklearn.save_model(lr, model_path)\n",
    "\n",
    "# Log the artifact              \n",
    "        mlflow.log_artifact(model_path)\n",
    "\n",
    "        best_run=run.info\n",
    "        return best_run\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d2b066-6db4-464e-b687-d5ddc5725bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%fs rm -r /Workspace/Users/ankur242199@exlservice.com/experiments/\n",
    "data.head()\n",
    "\n",
    "#%pip install mlflow[databricks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf73960-6577-441c-b6f4-da1c10a1978f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Call the model function\n",
    "train_sales_order_model(data, 0.01, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268ae873-6462-4019-bb48-b7dadad1315b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_model=train_sales_order_model(data, 0.65, 0.21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b73964-a4a3-4836-b69b-66b1509e4a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name=\"sales_order_model\"\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37666051-6bb3-4b96-aec4-44ff3b7dedd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def print_model_info(mod):\n",
    "    for i in mod:\n",
    "        print(\"name{}\".format(i.name))\n",
    "        print(\"version{}\".format(i.version))\n",
    "        print(\"run_id{}\".format(i.run_id))\n",
    "        print(\"current_stage{}\".format(i.current_stage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67bc8088-31e4-429d-b887-90db58422fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client=mlflow.tracking.MlflowClient()\n",
    "try:\n",
    "    client.create_registered_model(model_name)\n",
    "except Exception as e:\n",
    "    pass\n",
    "model_version=client.create_model_version(model_name,f\"{best_model.artifact_uri}/model\", best_model.run_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe4a0e3-a700-4e49-b5f3-545565917b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = \"dmpipeline-dev\"\n",
    "SCHEMA_NAME = \"poc\"\n",
    "MODEL_NAME = \"sales_order_model\"\n",
    "registered_model_name=f\"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}\"\n",
    "print(\"Registered model name:\", registered_model_name)\n",
    "\n",
    "model_x=client.get_latest_versions(name=registered_model_name)\n",
    "print_model_info(model_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76060e3c-e771-44d7-9262-3ed38a249ede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Listing all media and control variables with target and date column \n",
    "# -------------------------------\n",
    "media_cols = ['ASA_APP', 'BING_DISPLAY', 'BING_SEARCH',\n",
    "       'DV360_DISPLAY_OR_OLV', 'FACEBOOK_SOCIAL',\n",
    "       'GOOGLE_DISPLAY_OR_OLV', 'GOOGLE_SEARCH', 'LINKEDIN_SOCIAL', 'META_APP',\n",
    "       'REDDIT_SOCIAL', 'TAPTICA_APP', 'TWITTER_SOCIAL',\n",
    "       'COMMISSIONS_AFFILIATE', 'PLACEMENT_AFFILIATE', 'REDBOX_APP', 'LIFTOFF_APP', 'BRAND_SPEND']\n",
    "control_cols = ['NEWS_ANOMALY', 'HOLIDAY_FLAG', 'SALE_FLAG', 'WSJ_EMAILS_TOTAL']\n",
    "target_col = 'ORDERS'\n",
    "date_col = 'WEEK_START'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc800144-bc6b-4ccc-8a80-7066034a368d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Sorting data based on date column and creating new dataframes for media, control, sales and cost\n",
    "# -------------------------------\n",
    "Train_df = Train.sort_values(date_col).reset_index(drop = True)\n",
    "Test_df = Test.sort_values(date_col).reset_index(drop = True)\n",
    "\n",
    "# -------------------------------\n",
    "# Media variables\n",
    "# -------------------------------\n",
    "media_data_train = Train_df[media_cols].astype(float).to_numpy()\n",
    "media_data_test = Test_df[media_cols].astype(float).to_numpy()\n",
    "print(\"Media shapes:\", media_data_train.shape, media_data_test.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Control variables\n",
    "# -------------------------------\n",
    "control_data_train = Train_df[control_cols].astype(float).to_numpy()\n",
    "control_data_test  = Test_df[control_cols].astype(float).to_numpy()\n",
    "print(\"Control shapes:\", control_data_train.shape, control_data_train.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Target variable\n",
    "# -------------------------------\n",
    "target_train = Train_df[target_col].astype(float).to_numpy()\n",
    "target_test = Test_df[target_col].astype(float).to_numpy()\n",
    "print(\"Target shapes:\", target_train.shape, target_test.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Costs variables\n",
    "# -------------------------------\n",
    "costs_train = Train_df[media_cols].sum(axis = 0).to_numpy()\n",
    "costs_test = Test_df[media_cols].sum(axis = 0).to_numpy()\n",
    "print(\"Costs shapes:\", costs_train.shape, costs_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c625007-7221-44b7-a434-4905ad521a2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Define scalers\n",
    "# -------------------------------\n",
    "media_scaler   = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "control_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "target_scaler  = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "cost_scaler    = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "\n",
    "# -------------------------------\n",
    "# Scale media data (all columns)\n",
    "# -------------------------------\n",
    "media_data_train_scaled = media_scaler.fit_transform(media_data_train)\n",
    "media_data_test_scaled  = media_scaler.transform(media_data_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Scale only 3rd column of control data (index 3)\n",
    "# -------------------------------\n",
    "# Training\n",
    "control_data_train_scaled = control_data_train.copy()\n",
    "control_data_train_scaled[:, 3] = control_scaler.fit_transform(control_data_train[:, 3].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Test\n",
    "control_data_test_scaled = control_data_test.copy()\n",
    "control_data_test_scaled[:, 3] = control_scaler.transform(control_data_test[:, 3].reshape(-1, 1)).flatten()\n",
    "\n",
    "# -------------------------------\n",
    "# Scale target variable\n",
    "# -------------------------------\n",
    "target_train_scaled = target_scaler.fit_transform(target_train)\n",
    "target_test_scaled  = target_scaler.transform(target_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Scale costs (all columns)\n",
    "# -------------------------------\n",
    "cost_train_scaled = cost_scaler.fit_transform(costs_train)\n",
    "cost_test_scaled  = cost_scaler.transform(costs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7febafc-ef25-45d3-94c8-110e6692ba99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save all scalers locally\n",
    "joblib.dump(media_scaler, \"media_scaler.pkl\")\n",
    "joblib.dump(target_scaler, \"target_scaler.pkl\")\n",
    "joblib.dump(cost_scaler, \"cost_scaler.pkl\")\n",
    "joblib.dump(control_scaler, \"control_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d0f8a9b-fe03-403d-9ec3-f65281d56a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create or set an experiment\n",
    "exp_path=\"/Workspace/Users/ankur242199@exlservice.com/experiments/\"\n",
    "experiment_name = (path+ f\"lightweight_mmm_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=experiment_name, nested=True):\n",
    "    mlflow.log_artifact(\"media_scaler.pkl\", artifact_path=\"scalers\")\n",
    "    mlflow.log_artifact(\"target_scaler.pkl\", artifact_path=\"scalers\")\n",
    "    mlflow.log_artifact(\"cost_scaler.pkl\", artifact_path=\"scalers\")\n",
    "    mlflow.log_artifact(\"control_scaler.pkl\", artifact_path=\"scalers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b86fe12-c812-48cf-af09-81eed5fbf713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter search space\n",
    "model_names = [\"hill_adstock\"]\n",
    "warmup_values = [200, 300]\n",
    "samples_values = [400, 500]\n",
    "degree = [1, 2, 3]\n",
    "\n",
    "# -------------------------------\n",
    "# Training and Testing model performance on multiple warmup and sample values and logging using MLflow\n",
    "# -------------------------------\n",
    "for model_name, n_warmup, n_samples, n_deg in itertools.product(model_names, warmup_values, samples_values, degree):\n",
    "\n",
    "    # Creating run name using model name, warmpup and sample values\n",
    "    run_name = f\"{model_name}_warmup{n_warmup}_samples{n_samples}\"\n",
    "    \n",
    "    # Logging model name with hyperparameters and iteration name\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log params\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"n_warmup\", n_warmup)\n",
    "        mlflow.log_param(\"n_samples\", n_samples)\n",
    "        mlflow.log_param(\"n_degree\", n_deg)\n",
    "        mlflow.log_param(\"n_media_channels\", media_cols)\n",
    "        \n",
    "        # Train the model\n",
    "        mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n",
    "        mmm.fit(\n",
    "            media=media_data_train_scaled,\n",
    "            media_prior=cost_train_scaled,\n",
    "            target=target_train_scaled,\n",
    "            extra_features=control_data_train_scaled,\n",
    "            number_warmup=n_warmup,\n",
    "            number_samples=n_samples,\n",
    "            number_chains=4,\n",
    "            degrees_seasonality=n_deg,\n",
    "            seasonality_frequency=52,\n",
    "            seed=1\n",
    "        )\n",
    "        \n",
    "        # Predict on Train data and doing inverse transform\n",
    "        target_train = mmm._target\n",
    "        posterior_pred = mmm.trace[\"mu\"]\n",
    "        posterior_pred = target_scaler.inverse_transform(posterior_pred)\n",
    "        target_train = target_scaler.inverse_transform(target_train)\n",
    "\n",
    "        # Calculating Train R2\n",
    "        r2_train = az.r2_score(y_true=target_train, y_pred=posterior_pred)\n",
    "        train_r2 = r2_train.iloc[0]\n",
    "\n",
    "        # Calculating Train MAPE\n",
    "        train_mape = 100 * metrics.mean_absolute_percentage_error(y_true=target_train, y_pred=posterior_pred.mean(axis=0))\n",
    "\n",
    "        # Calculating Train RMSE\n",
    "        train_rmse = np.sqrt(metrics.mean_squared_error(\n",
    "            y_true=target_train, \n",
    "            y_pred=posterior_pred.mean(axis=0)\n",
    "        ))\n",
    "\n",
    "        # Logging Train artifacts and model\n",
    "        mlflow.log_metric(\"train_R2\", train_r2)\n",
    "        mlflow.log_metric(\"train_mape\", train_mape)\n",
    "        mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "        mlflow.sklearn.log_model(mmm, \"model\")\n",
    "\n",
    "        # Predict on Test data (scaled)\n",
    "        posterior_pred_test = mmm.predict(\n",
    "            media=media_data_test_scaled,\n",
    "            extra_features=control_data_test_scaled\n",
    "        )\n",
    "\n",
    "        # Inverse transform predictions\n",
    "        posterior_pred_test_unscaled = target_scaler.inverse_transform(posterior_pred_test)\n",
    "\n",
    "        # Calculation Test R2\n",
    "        r2_test = az.r2_score(\n",
    "            y_true=target_test,\n",
    "            y_pred=posterior_pred_test_unscaled\n",
    "        )\n",
    "        test_r2 = r2_test.iloc[0]\n",
    "\n",
    "        # Calculating Test MAPE\n",
    "        test_mape = 100 * metrics.mean_absolute_percentage_error(\n",
    "            y_true=target_test,\n",
    "            y_pred=posterior_pred_test_unscaled.mean(axis=0)\n",
    "        )\n",
    "\n",
    "        # Calculating Test RMSE\n",
    "        test_rmse = np.sqrt(metrics.mean_squared_error(\n",
    "            y_true=target_test, \n",
    "            y_pred=posterior_pred_test_unscaled.mean(axis=0)\n",
    "        ))\n",
    "\n",
    "        # Logging Test artifacts\n",
    "        mlflow.log_metric(\"test_R2\", test_r2)\n",
    "        mlflow.log_metric(\"test_mape\", test_mape)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd92a56-bd20-4c8e-862d-cabd38ad80b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d4e9e2-31e6-44f8-a64f-723e8efa17fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Load particular experiment runs:\n",
    "# -------------------------------\n",
    "print(experiment_name)\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# -------------------------------\n",
    "# Listing all iterations under given experiment id in pandas dataframe:# -------------------------------\n",
    "# -------------------------------\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    output_format=\"pandas\")\n",
    "# -------------------------------\n",
    "# Ensure numeric conversion for all required metrics:\n",
    "# -------------------------------\n",
    "for col in [\"metrics.train_r2\", \"metrics.test_r2\", \"metrics.train_mape\", \"metrics.test_mape\",\n",
    "            \"metrics.train_rmse\", \"metrics.test_rmse\"]:\n",
    "    if col in runs_df.columns:\n",
    "        runs_df[col] = pd.to_numeric(runs_df[col], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25a4441-17f8-438f-8017-b5492f069125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Below logic is to select the best model from all the iterations\n",
    "# 1. Keep only models where the absolute difference between train R² and test R² is ≤ 5% of train R²\n",
    "# 2. a. From these, pick the model with the highest train R²\n",
    "#    b. If there are no models in Step 1, Find models whose train R² is within 95% of the maximum train R².\n",
    "#       Among these, pick the model with the smallest relative drop from train R² to test R².\n",
    "# -------------------------------\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Filter models with abs(train R2 - test R2) < 5% of train R2\n",
    "# -------------------------------\n",
    "runs_df[\"r2_diff_pct\"] = abs(runs_df[\"metrics.train_R2\"] - runs_df[\"metrics.test_R2\"]) / runs_df[\"metrics.train_R2\"] * 100\n",
    "\n",
    "filtered_candidates = runs_df[runs_df[\"r2_diff_pct\"] <= 5].copy()\n",
    "\n",
    "if not filtered_candidates.empty:\n",
    "    # Step 2a: Pick model with maximum train R²\n",
    "    best_run = filtered_candidates.loc[filtered_candidates[\"metrics.train_R2\"].idxmax()]\n",
    "else:\n",
    "    # -------------------------------\n",
    "    # Step 2b: Fallback: previous logic\n",
    "    # -------------------------------\n",
    "    max_train_r2 = runs_df[\"metrics.train_R2\"].max()\n",
    "    threshold = max_train_r2 * 0.95  # within 5% of max\n",
    "\n",
    "    candidates = runs_df[runs_df[\"metrics.train_R2\"] >= threshold].copy()\n",
    "    candidates[\"r2_drop_pct\"] = ((candidates[\"metrics.train_R2\"] - candidates[\"metrics.test_R2\"]) / candidates[\"metrics.train_R2\"]) * 100\n",
    "\n",
    "    best_run = candidates.loc[candidates[\"r2_drop_pct\"].idxmin()]\n",
    "\n",
    "# -------------------------------\n",
    "# Display the final selected run\n",
    "# -------------------------------\n",
    "print(\"Final selected run:\")\n",
    "print(best_run[[\"run_id\", \"metrics.train_R2\", \"metrics.test_R2\", \"r2_diff_pct\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1526e941-ccf8-4b39-8e1d-f59d56cb659a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_id = best_run['run_id']\n",
    "\n",
    "# Load back the model\n",
    "loaded_model = mlflow.sklearn.load_model(f\"runs:/{run_id}/model\")\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae80f5c5-2f0c-48cc-827b-b019df20999d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Plotting Train results and metrics\n",
    "# -------------------------------\n",
    "\n",
    "plot.plot_model_fit(loaded_model, target_scaler=target_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cac56ff5-2eb7-46af-bbc7-f958a31d522f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Plotting Test results and metrics\n",
    "# -------------------------------\n",
    "posterior_pred_test = loaded_model.predict(\n",
    "    media=media_data_test_scaled,\n",
    "    extra_features=control_data_test_scaled\n",
    ")\n",
    "\n",
    "# Inverse transform predictions & actuals\n",
    "posterior_pred_test_unscaled = target_scaler.inverse_transform(posterior_pred_test)\n",
    "plot.plot_out_of_sample_model_fit(out_of_sample_predictions=posterior_pred_test_unscaled,\n",
    "                                  out_of_sample_target=target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c124544b-2a37-40d8-bb46-08b8db209556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3693a667-eb37-4075-aecc-75ca2c740f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
